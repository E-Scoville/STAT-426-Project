{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stat 426 Project**  \n",
    "## ML Analysis of Pew Research Data  \n",
    "### Eirik Scoville"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**  \n",
    "  \n",
    "For this project, I am using data from the Pew Research Center 2014 U.S. Religious Landscape Study to try and build a classification model that can predict a person's religious affiliation based on their views of several social, political, and theological issues.  \n",
    "  \n",
    "The data to be used consists of two separate datasets--one for data from the continental US, and the other containing survey answers from Alaska and Hawaii. The first dataset contains 35,556 observations of 135 variables (including survey responses), and the second contains 401 observations of 135, making the final combined dataset size 35,957x135."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize packages\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "con = pd.read_spss('Religious Landscape Survey Data - Continental US.sav')\n",
    "aah = pd.read_spss('Religious Landscape Survey Data - Alaska and Hawaii.sav')\n",
    "\n",
    "# Combine into one dataset\n",
    "df = pd.concat([con, aah], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EDA and Feature Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently too many features to see in either an info call or by printing the head of the data, so as a first step, we must manually look at the features and determine which of them if any may be dropped as superfluous to our analysis. First, we will look at features that were not part of the survey questions themselves. These features are  \n",
    "- weight,  \n",
    "- psraid (a unique id number),  \n",
    "- int_date (date of the interview),  \n",
    "- lang (language of the interview),  \n",
    "- type (type of sample used),  \n",
    "- cregion (census region),  \n",
    "- state,  \n",
    "- usr (community type),  \n",
    "- usr1 (redundant),  \n",
    "- form,  \n",
    "- density3 (population density quintiles),  \n",
    "- marital (marital status),  \n",
    "- hisp (is the respondent hispanic),  \n",
    "- race,  \n",
    "- chr (is the respondent Christian),  \n",
    "- denom (specific religious affiliation),  \n",
    "- family (religious affiliation group),  \n",
    "- reltrad (religious affiliation tradition),  \n",
    "- protfam (subsets of Protestantism),  \n",
    "- children (does the respondant have any children under 18),  \n",
    "- sex,  \n",
    "- age,  \n",
    "- educ (education level),  \n",
    "- income,  \n",
    "- regist (is the respondent registered to vote),  \n",
    "- regicert (is the respondent certain that they are registered),  \n",
    "- party (Republican, Democrat, or Independent),  \n",
    "- partyln (does the respondent lean more Rep. or Dem.),  \n",
    "- ideo (conservative or liberal ideology),  \n",
    "- pvote04a (did the respondent vote in 2004),  \n",
    "- pvote04b (did the respondent vote for Bush, Kerry, or someone else)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right off the bat, there are a few of these variables that I will drop. Weight is only used for Pew's own analysis, and won't be helpful to me. The unique id assigned will not translate into any useful information ML analysis, so it will be dropped as well. The interview date, form, children, registration, '04 voting history, and type features are unlikely to be related to a person's religion. The language feature has only two values (English and a handful of Spanish), so I've chosen to drop it because the Spanish responses only account for about 3% of the total. Finally, there are several variables which represent different levels of granularity about the respondent's religious affiliation. One of these will be the label that we are trying to predict, but we must drop the others, as they will be too highly correlated with the outcome. After deliberation, 'reltrad' will be the value we will use as the prediction label, which has 16 possible values. This will likely need to be trimmed down for a good prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35957, 116)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of variables to drop\n",
    "drop = ['denom','protfam','family','weight','psraid','int_date','lang','type','usr','form','marital','children','sex','age','regist','regicert','pvote04a','pvote04b','chr']\n",
    "df = df.drop(drop, axis=1)\n",
    "# New size of dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, there are a few of the actual survey questions that will need to be dropped: the ones that actually ask about the respondents' religious affiliation. Additionally, there are a few questions which gather information that we've determined to be unecessary for this analysis. There are also some features in which the majority of the observations are missing. These are removed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35957, 87)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of questions to drop\n",
    "drop2 = ['q17','q17a','q17b','q17c','q17d','q17e','q17f','q17g','q17h','q17i','q17j','q17k','q17l','q17m','q17n','q17o','q17p','q17q','q17r','q17s','q17t','q17u','q17v','q50','q50a','q50b','q51','q52','q53']\n",
    "df = df.drop(drop2, axis=1)\n",
    "# New size of dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some questions are directed more at one group of people than another, and those tend to have lots of missing values. Rather than remove those, it will suffice to impute a placeholder string in any question where a response was not recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all NaNs with placeholder string\n",
    "df = df.astype(str)\n",
    "df = df.replace(np.nan, 'No entry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of this dataset consists of survey answers, which are actual text fields, and therefore cannot really be visualized in a graphical way. It's also hard to display a header with so many variables. Instead, here is a glimpse of the text of just some of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q18</th>\n",
       "      <th>q27</th>\n",
       "      <th>q34</th>\n",
       "      <th>q36</th>\n",
       "      <th>q41</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Between 100 and 500</td>\n",
       "      <td>Absolutely certain</td>\n",
       "      <td>Yes</td>\n",
       "      <td>A few times a week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>nan</td>\n",
       "      <td>Fairly certain</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Seldom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>nan</td>\n",
       "      <td>Fairly certain</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Seldom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nan</td>\n",
       "      <td>Less than 100</td>\n",
       "      <td>Absolutely certain</td>\n",
       "      <td>No</td>\n",
       "      <td>A few times a week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>No</td>\n",
       "      <td>Seldom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>No</td>\n",
       "      <td>Never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>No</td>\n",
       "      <td>Never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Don't Know/Refused (VOL.)</td>\n",
       "      <td>nan</td>\n",
       "      <td>Not at all certain</td>\n",
       "      <td>No</td>\n",
       "      <td>Never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Less than 100</td>\n",
       "      <td>Absolutely certain</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Several times a day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nan</td>\n",
       "      <td>Between 500 and 2,000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No</td>\n",
       "      <td>A few times a month</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         q18                    q27                 q34  q36  \\\n",
       "0                        Yes    Between 100 and 500  Absolutely certain  Yes   \n",
       "1                        Yes                    nan      Fairly certain  Yes   \n",
       "2                         No                    nan      Fairly certain  Yes   \n",
       "3                        nan          Less than 100  Absolutely certain   No   \n",
       "4                        nan                    nan                 nan   No   \n",
       "5                        nan                    nan                 nan   No   \n",
       "6                        nan                    nan                 nan   No   \n",
       "7  Don't Know/Refused (VOL.)                    nan  Not at all certain   No   \n",
       "8                        Yes          Less than 100  Absolutely certain  Yes   \n",
       "9                        nan  Between 500 and 2,000                 nan   No   \n",
       "\n",
       "                   q41  \n",
       "0   A few times a week  \n",
       "1               Seldom  \n",
       "2               Seldom  \n",
       "3   A few times a week  \n",
       "4               Seldom  \n",
       "5                Never  \n",
       "6                Never  \n",
       "7                Never  \n",
       "8  Several times a day  \n",
       "9  A few times a month  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['q18','q27','q34','q36','q41']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed the data to a ML algorithm, the values in each observation must be numeric rather than text, as they are now. The following code attempts to systematically encode each of the 87 remaining features in groups based on the type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# To save memory, many ordinal questions will be encoded using label encoding instead of one hot encoding.\n",
    "# First, we must specify which questions belong in this group\n",
    "labencode = df.drop('reltrad', axis=1).columns\n",
    "\n",
    "# Instantiate the encoder\n",
    "le = LabelEncoder()\n",
    "# Perform the transformation\n",
    "df[labencode] = df[labencode].apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will split the data into a training set on which to train the models, and a test set on which to validate the performance of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=.3, stratify=df.reltrad, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop('reltrad', axis=1)\n",
    "X_test = test.drop('reltrad', axis=1)\n",
    "\n",
    "y_train = train['reltrad']\n",
    "y_test = test['reltrad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since every question has a different number of possible answers, and to facilitate the following computations, we will take the numeric values in the dataframe and scale them all to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Instantiate scaler\n",
    "minmax = MinMaxScaler()\n",
    "# Fit scaler to training data\n",
    "mmtrans = minmax.fit(X_train)\n",
    "# Transform both training and test sets\n",
    "X_train = minmax.transform(X_train)\n",
    "X_test = minmax.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine whether an effective model can be built to classify the religion of survey respondents, three different ML algorithms will be tried: A K-Nearest Neighbors classifier, a Random Forest classifier, and a Gradient Boost classifier. First, we have the KNN classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n",
       "             param_grid={'n_neighbors': [11, 21, 31, 41, 51, 101]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Build a KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "parameters = {\n",
    "    'n_neighbors': [11,21,31,41,51,101]\n",
    "}\n",
    "knncv = GridSearchCV(knn, parameters, cv=5)\n",
    "knncv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 41}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See which k had the best results\n",
    "knncv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5943\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 41)\n",
    "knn.fit(X_train, y_train)\n",
    "yhatknn = (knn.predict(X_test))\n",
    "print(\"Accuracy: \" + str(round(accuracy_score(y_test, yhatknn),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the KNN classifier did not perform spectacularly well. Next, we will try a Random Forest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(n_jobs=-1),\n",
       "             param_grid={'max_depth': [5, 10, 50, 100, None],\n",
       "                         'n_estimators': [100, 250, 500]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Build a Random Forest model\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "parameters = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [5, 10, 50, 100, None]\n",
    "}\n",
    "\n",
    "rfcv = GridSearchCV(rf, parameters, cv=5)\n",
    "rfcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'max_depth': None, 'n_estimators': 500}\n",
      "\n",
      "0.78 (+/-0.002) for {'max_depth': 5, 'n_estimators': 100}\n",
      "0.78 (+/-0.002) for {'max_depth': 5, 'n_estimators': 250}\n",
      "0.779 (+/-0.003) for {'max_depth': 5, 'n_estimators': 500}\n",
      "0.837 (+/-0.008) for {'max_depth': 10, 'n_estimators': 100}\n",
      "0.838 (+/-0.007) for {'max_depth': 10, 'n_estimators': 250}\n",
      "0.838 (+/-0.009) for {'max_depth': 10, 'n_estimators': 500}\n",
      "0.851 (+/-0.003) for {'max_depth': 50, 'n_estimators': 100}\n",
      "0.852 (+/-0.007) for {'max_depth': 50, 'n_estimators': 250}\n",
      "0.851 (+/-0.007) for {'max_depth': 50, 'n_estimators': 500}\n",
      "0.849 (+/-0.006) for {'max_depth': 100, 'n_estimators': 100}\n",
      "0.851 (+/-0.005) for {'max_depth': 100, 'n_estimators': 250}\n",
      "0.851 (+/-0.007) for {'max_depth': 100, 'n_estimators': 500}\n",
      "0.848 (+/-0.008) for {'max_depth': None, 'n_estimators': 100}\n",
      "0.851 (+/-0.007) for {'max_depth': None, 'n_estimators': 250}\n",
      "0.852 (+/-0.006) for {'max_depth': None, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "# Print results of the tuning\n",
    "def print_results(results):\n",
    "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
    "\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))\n",
    "\n",
    "print_results(rfcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=None, n_estimators=500, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "yhatrf = (rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8554\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" + str(round(accuracy_score(y_test, yhatrf),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest model performed much better than the KNN model did. As a final check, we'll perform a Gradient Boost model as well using the Adaboost package, and see if it can improve on the score of the Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=AdaBoostClassifier(),\n",
       "             param_grid={'learning_rate': [0.1, 0.01, 0.001],\n",
       "                         'n_estimators': [100, 250, 500]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Create an Adaptive Boosting model\n",
    "ada = AdaBoostClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'learning_rate': [.1, .01, .001]\n",
    "}\n",
    "gbcv = GridSearchCV(ada, parameters, cv=5)\n",
    "gbcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.001, 'n_estimators': 250}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(learning_rate=0.001, n_estimators=250)\n",
    "ada.fit(X_train, y_train)\n",
    "yhatgb = ada.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.661\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" + str(round(accuracy_score(y_test, yhatgb),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the Adaptive Boosting algorithm did better than the KNN model, but not as well as the Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that performed the poorest for this analysis was the K Nearest Neighbor algorithm, consistently scoring somewhere around 59% accuracy. The next best performing model was the Adaptive Boosting algorithm, scoring around 66% accuracy. And finally, the Random Forest algorithm knocked it out of the park with an impressive 85% accuracy. This outperformed my own expectations, because there were quite a few categories in the response label, and usually the more categories you have, the harder it is to get really good accuracy. I was expecting to have to combine some of the response categories into macro categories, but with an accuracy of 85%, it almost feels unnecessary to do so. A full list of the categories being predicted follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Evangelical Protestant Churches                                 9569\n",
       " Catholic                                                        8126\n",
       " Mainline Protestant Churches                                    7550\n",
       " Unaffiliated                                                    5141\n",
       " Historically Black Protestant Churches                          2000\n",
       " Jewish                                                           683\n",
       " Mormon                                                           599\n",
       " Other Faiths                                                     460\n",
       " Buddhist                                                         423\n",
       " Orthodox                                                         367\n",
       " Don’t know/refused (no information on religious affiliation)     274\n",
       " Hindu                                                            258\n",
       " Jehovah's Witness                                                218\n",
       " Other Christian                                                  130\n",
       " Muslim                                                           117\n",
       " Other World Religions                                             42\n",
       "Name: reltrad, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reltrad'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that there were so many of certain groups and so few of some other groups may have skewed the prediction algorithm somewhat, introducing bias and lowering the overall accuracy scores. This also could be addressed by combining some of the smaller religious groups into a larger macro-category, and would be a good jumping-off point for further analysis in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, with the overwhelming amount of data available, good results were able to be acheived with minimal feature engineering. Considering the number of features excluded from the analysis, and the use of label encoding rather than one hot encoding, there is good reason to believe that including more of that data would result in even better results. Unfortunately, I could not explore those options because of hardware and memory constraints. But even so, I think it's safe to conclude that the Pew Research survey answers do give a very good indication of which religion a person belongs to, based on demographic, social, and political issues. It serves as a testament to how diverse smaller branches of the same religion. And yet, the members of those groups tend to share enough in common to identify themselves as part of a certain group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
